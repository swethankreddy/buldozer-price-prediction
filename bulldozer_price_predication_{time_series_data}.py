# -*- coding: utf-8 -*-
"""Bulldozer Price Predication (Time series data)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w86qp0tspS1PVNgkY5brgwcBbL6dq9em
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

data = pd.read_csv("/content/drive/MyDrive/BullDozer price prediction/Train.csv")

data

"""The evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices."""

data.info()

data.isna().sum()
#checking for missing values

fig,ax = plt.subplots()
ax.scatter(data["saledate"][:1000], data["SalePrice"][:1000])

"""When working with time series data, it's a good idea to make sure any data is in the format of a datetime object (a Python data type which encodes specific information about dates).

"""

data_= pd.read_csv("/content/drive/MyDrive/BullDozer price prediction/TrainAndValid.csv",
                 low_memory=False,
                 parse_dates=["saledate"])

data_.info()

data["saledate"]

#After using parse_dates
data_["saledate"]

fig,ax = plt.subplots()
ax.scatter(data_["saledate"][:1000], data_["SalePrice"][:1000])

fig,ax = plt.subplots()
ax.scatter(data_["saledate"], data_["SalePrice"])

"""Sorting DataFrame by saledate
when working on a time series problem and trying to predict future examples given past examples,sorting our data by date simplifies our analysis
"""

data_.sort_values(by=["saledate"], inplace=True, ascending=True)
data_["saledate"].head(20)

data_["saledate"].dtype

data_tmp = data_.copy()

"""datetime64 and timedelta64 are 2 different types of datatypes when working with time series data , these datatypes has many attributes which you search and get from google according to to requirement"""

data_tmp[:1] ##Not sure why can't i access individual row

data_tmp.iloc[0] ##TO access individual row

#data_tmp.iloc[0]["saledate"].dt.year

data_tmp[:1]["saledate"].dt.year

data_tmp[3000:3001]["saledate"].dt.month

"""Adding datetime parameters for saledate

"""

data_tmp["saleYear"] = data_tmp.saledate.dt.year
data_tmp["saleMonth"] = data_tmp.saledate.dt.month
data_tmp["saleDay"] = data_tmp.saledate.dt.day
data_tmp["saleDayofweek"] = data_tmp.saledate.dt.dayofweek
data_tmp["saleDayofyear"] = data_tmp.saledate.dt.dayofyear

#Removing the date column
data_tmp.drop("saledate", axis=1, inplace=True)

data_tmp["state"].value_counts()

categorical_features = [i for i in data_tmp.columns if data_tmp[i].dtypes =='O']

len(categorical_features)

categorical_features

data_tmp[categorical_features].T

"""Assigning dummies to categorical values .

There are 3 methods

1.Create Dummies and concanate them

2.one hot encoding

3.Using pandas types API  
"""

#method 1
dummies = pd.get_dummies(data_tmp[categorical_features])

dummies.shape

data_tmp = pd.concat([data_tmp, dummies], axis=1)

data_tmp = data_tmp.drop(categorical_features,axis=1)

data_tmp

#method 2
data_["saleYear"] = data_.saledate.dt.year
data_["saleMonth"] = data_.saledate.dt.month
data_["saleDay"] = data_.saledate.dt.day
data_["saleDayofweek"] = data_.saledate.dt.dayofweek
data_["saleDayofyear"] = data_.saledate.dt.dayofyear

data_.drop("saledate", axis=1, inplace=True)


for label, content in data_.items():
    if pd.api.types.is_string_dtype(content):
        data_[label] = content.astype("category").cat.as_ordered()

data_.info()

data_.state.cat.categories

data_.state.cat.codes

data_.isnull().sum()/len(data_)

# Saving preprocessed data
#data_.to_csv("/content/drive/MyDrive/BullDozer price prediction/data_.csv",
 #             index=False)

data_ = pd.read_csv("/content/drive/MyDrive/BullDozer price prediction/data_.csv",low_memory = False)

data_

data_.isna().sum()

"""Filling missing values"""

for label, content in data_.items():
    if pd.api.types.is_numeric_dtype(content):
        print(label)

for label, content in data_.items():
  print(pd.isnull(content))

for label, content in data_.items():
  if pd.api.types.is_numeric_dtype(content):
    print(pd.isnull(content).sum())

for label, content in data_.items():
    if pd.api.types.is_numeric_dtype(content):
        if pd.isnull(content).sum():
            print(label)

for label, content in data_.items():
    if pd.api.types.is_numeric_dtype(content):
        if pd.isnull(content).sum():
            # Add a binary column which tells if the data was missing our not
            data_[label+"_is_missing"] = pd.isnull(content) #Tells true or false in added column
            # Fill missing numeric values with median since it's more robust than the mean
            data_[label] = content.fillna(content.median())

# Checking if there's any null values
for label, content in data_.items():
    if pd.api.types.is_numeric_dtype(content):
        if pd.isnull(content).sum():
            print(label)

# Checking to see how many examples were missing initially
data_.auctioneerID_is_missing.value_counts()

#Printing catogerical columns
for label, content in data_.items():
    if not pd.api.types.is_numeric_dtype(content):
        print(label)

pd.Categorical(data_["UsageBand"]).codes #It shows -1

# Turning categorical variables into numbers
for label, content in data_.items():
    # Check columns which *aren't* numeric
    if not pd.api.types.is_numeric_dtype(content):
        # Add binary column to inidicate whether sample had missing value
        data_[label+"_is_missing"] = pd.isnull(content)
        # We add the +1 because pandas encodes missing categories as -1
        data_[label] = pd.Categorical(content).codes+1

pd.Categorical(data_["UsageBand"]).codes

data_.UsageBand.value_counts()

data_.info()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.ensemble import RandomForestRegressor
# 
# model = RandomForestRegressor( n_jobs=-1,random_state = 42)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model.fit(data_.drop("SalePrice", axis=1), data_tmp.SalePrice)

model.score(data_.drop("SalePrice", axis=1), data_tmp.SalePrice)

data_["saleYear"]

# Split data into training and validation
data_val = data_[data_.saleYear == 2012]
data_train = data_tmp[data_.saleYear != 2012]

len(data_val), len(data_train)

x_train = data_train.drop(["SalePrice"],axis = 1)
y_train = data_train["SalePrice"]
x_test = data_val.drop(["SalePrice"],axis = 1)
y_test = data_val["SalePrice"]

"""Evaluation function is  R:oot mean squared log error (RMSLE).

"""

# Createing  evaluation function (the competition uses Root Mean Square Log Error)
from sklearn.metrics import mean_squared_log_error, mean_absolute_error

def rmsle(y_test, y_preds):
    return np.sqrt(mean_squared_log_error(y_test, y_preds))

# Create function to evaluate our model
def show_scores(model):
    train_preds = model.predict(x_train)
    val_preds = model.predict(x_valid)
    scores = {"Training MAE": mean_absolute_error(y_train, train_preds),
              "Valid MAE": mean_absolute_error(y_valid, val_preds),
              "Training RMSLE": rmsle(y_train, train_preds),
              "Valid RMSLE": rmsle(y_valid, val_preds),
              "Training R^2": model.score(x_train, y_train),
              "Valid R^2": model.score(x_valid, y_valid)}
    return scores

show_scores(model)

"""Tuning hyperparameters using randimosedSearchCV"""

model = RandomForestRegressor(n_jobs=-1,
                              max_samples=10000)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Cutting down the max number of samples each tree can see improves training time
# model.fit(x_train, y_train)

show_scores(model)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.model_selection import RandomizedSearchCV
# rf_grid = {"n_estimators": np.arange(10, 100, 10),
#            "max_depth": [None, 3, 5, 10],
#            "min_samples_split": np.arange(2, 20, 2),
#            "min_samples_leaf": np.arange(1, 20, 2),
#            "max_features": [0.5, 1.0, "sqrt"],
#            "max_samples": [100000]}
#

rs_model = RandomizedSearchCV(RandomForestRegressor(),
                              param_distributions=rf_grid,
                              n_iter=200,
                              cv=5,
                              verbose=True)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rs_model.fit(x_train, y_train)

best_model = rs_model.best_params_

test = pd.read_csv("/content/drive/MyDrive/BullDozer price prediction/Test.csv", parse_dates=["saledate"])

test

"""Preprocessing the test data in the shape of pre processed training data"""

Creating a function to preprocess

def preprocess_data(df):
    # Add datetime parameters for saledate
    df["saleYear"] = df.saledate.dt.year
    df["saleMonth"] = df.saledate.dt.month
    df["saleDay"] = df.saledate.dt.day
    df["saleDayofweek"] = df.saledate.dt.dayofweek
    df["saleDayofyear"] = df.saledate.dt.dayofyear

    # Drop original saledate
    df.drop("saledate", axis=1, inplace=True)

    # Fill numeric rows with the median
    for label, content in df.items():
        if pd.api.types.is_numeric_dtype(content):
            if pd.isnull(content).sum():
                df[label+"_is_missing"] = pd.isnull(content)
                df[label] = content.fillna(content.median())

        # Turn categorical variables into numbers
        if not pd.api.types.is_numeric_dtype(content):
            df[label+"_is_missing"] = pd.isnull(content)
            # We add the +1 because pandas encodes missing categories as -1
            df[label] = pd.Categorical(content).codes+1
      return df

test = preprocess_data(test)

test

test_preds = best_model.predict(test)

# We can find how the columns differ using sets
set(x_train.columns) - set(test.columns)

# Match test dataset columns to training dataset
test["auctioneerID_is_missing"] = False
test.head()

test_preds = best_model.predict(test)





